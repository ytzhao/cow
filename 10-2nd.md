### 2017-10-09 Mon
[CUDA tutorial][1]
[CUDA online tutorial from NVIDIA][2]

**1. Where is libSensorSys.so? (need to check with Gu Qiyuan)** 
> - Bin/NewSensor
> - Bin/Debug(Release)/NewSensor



**2. link libraries *libDMTrans.so* & *libLayerFilter.so* are programmed by Liu Yehang and both are GPU related.**



**CUDA (Compute Unified Devices Architecture)**



**3. what is .cu file? **
> .cu file is a developer file written for CUDA, and NVIDIA architecture created for parallel processing on NVIDIA GPUs, .cu files are called "kernels", which are processing units that can run in parallel on NVIDIA's CUDA-based GPUs. Kernels are compiles with **nvcc**, a compiler program included with the NVIDIA CUDA Toolkit.



**4. CUDA tutorial**
The extention to the C programming language are four-fold: 

> **Function** type qualifiers to specify whether a function executes on the host or on the device and whether it is callable from the host or from the device **\_\_global\_\_, \_\_device\_\_, \_\_host\_\_**. 

> **Variable** type qulifiers to specify the *memory location* of a variable **\_\_device\_\_, \_\_shared\_\_**. 

> Four build-in variables that specify the **grid and block dimensions** and the **block and thread indices**: **gridDim, blockIdx, blockDim, threadIdx**.



**5. Steps in CUDA code**
> **Host code (xx.cpp & xx.cu): **
>
> > 1. Initialize/acquire device (GPU)
> > 2. Allocate memory on GPU
> > 3. Copy data from host to GPU
> > 4. Execute kernel on GPU
> > 5. Copy data from GPU to host
> > 6. Deallocate memory on GPU
> > 7. Release device
> > 8. Run "Gold" version on host
> > 9. Compare results



**6. Kernel code (xx_kernel.cu): **
A kernel is a function callable from the host and executed on the CUDA device -- simultaneously by many threads in parllel. How to call a kernel involves **specify the name of the kernel** and **an execution configuration**. An execution configuration is just defining **the number of parallel threads in a group** and **the number of groups to use** when running the kernel for the CUDA device.



**7. NVIDIA CUDA programming basics: **
> - Programming model
> - Memory model
> - CUDA API basics



**8. CUDA Programming model**
The resulting program is called a **Kernel**, the batch of threads that executes a kernel is organized as a grid of thread blocks.



**9. Memory model**



[1]:http://geco.mines.edu/tesla/cuda_tutorial_mio/
[2]:http://www.nvidia.cn/object/cuda_education_cn_old.html